{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = MNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(training_data, batch_size=4, shuffle=True,num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(784, 200)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200, 10)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "\n",
    "# optim = Adam(tinymodel.parameters(),lr=1e-9)\n",
    "optim = SGD(tinymodel.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.309517  [    4/60000]\n",
      "loss: 2.281843  [  404/60000]\n",
      "loss: 2.167447  [  804/60000]\n",
      "loss: 1.947353  [ 1204/60000]\n",
      "loss: 1.505543  [ 1604/60000]\n",
      "loss: 1.688819  [ 2004/60000]\n",
      "loss: 1.941329  [ 2404/60000]\n",
      "loss: 1.731002  [ 2804/60000]\n",
      "loss: 2.353802  [ 3204/60000]\n",
      "loss: 2.022069  [ 3604/60000]\n",
      "loss: 1.895247  [ 4004/60000]\n",
      "loss: 1.530955  [ 4404/60000]\n",
      "loss: 1.462959  [ 4804/60000]\n",
      "loss: 1.717743  [ 5204/60000]\n",
      "loss: 1.680048  [ 5604/60000]\n",
      "loss: 1.645811  [ 6004/60000]\n",
      "loss: 1.551320  [ 6404/60000]\n",
      "loss: 1.476107  [ 6804/60000]\n",
      "loss: 1.480225  [ 7204/60000]\n",
      "loss: 1.795961  [ 7604/60000]\n",
      "loss: 1.712790  [ 8004/60000]\n",
      "loss: 1.496665  [ 8404/60000]\n",
      "loss: 1.948172  [ 8804/60000]\n",
      "loss: 1.506967  [ 9204/60000]\n",
      "loss: 1.684376  [ 9604/60000]\n",
      "loss: 1.464545  [10004/60000]\n",
      "loss: 1.462043  [10404/60000]\n",
      "loss: 1.473217  [10804/60000]\n",
      "loss: 1.508594  [11204/60000]\n",
      "loss: 1.472673  [11604/60000]\n",
      "loss: 1.466530  [12004/60000]\n",
      "loss: 1.471798  [12404/60000]\n",
      "loss: 1.497318  [12804/60000]\n",
      "loss: 1.556956  [13204/60000]\n",
      "loss: 1.875351  [13604/60000]\n",
      "loss: 1.602086  [14004/60000]\n",
      "loss: 1.620927  [14404/60000]\n",
      "loss: 1.461365  [14804/60000]\n",
      "loss: 1.521171  [15204/60000]\n",
      "loss: 1.619593  [15604/60000]\n",
      "loss: 1.888524  [16004/60000]\n",
      "loss: 1.713998  [16404/60000]\n",
      "loss: 1.464763  [16804/60000]\n",
      "loss: 1.468595  [17204/60000]\n",
      "loss: 1.711116  [17604/60000]\n",
      "loss: 1.467183  [18004/60000]\n",
      "loss: 1.693807  [18404/60000]\n",
      "loss: 1.461413  [18804/60000]\n",
      "loss: 1.498858  [19204/60000]\n",
      "loss: 1.483383  [19604/60000]\n",
      "loss: 1.465057  [20004/60000]\n",
      "loss: 1.472306  [20404/60000]\n",
      "loss: 1.463493  [20804/60000]\n",
      "loss: 1.503354  [21204/60000]\n",
      "loss: 1.474603  [21604/60000]\n",
      "loss: 1.711126  [22004/60000]\n",
      "loss: 1.675219  [22404/60000]\n",
      "loss: 1.462244  [22804/60000]\n",
      "loss: 1.755663  [23204/60000]\n",
      "loss: 1.951576  [23604/60000]\n",
      "loss: 1.713562  [24004/60000]\n",
      "loss: 1.685708  [24404/60000]\n",
      "loss: 1.489088  [24804/60000]\n",
      "loss: 1.630392  [25204/60000]\n",
      "loss: 1.708506  [25604/60000]\n",
      "loss: 1.465786  [26004/60000]\n",
      "loss: 1.555629  [26404/60000]\n",
      "loss: 1.956185  [26804/60000]\n",
      "loss: 1.469824  [27204/60000]\n",
      "loss: 1.523161  [27604/60000]\n",
      "loss: 1.462626  [28004/60000]\n",
      "loss: 1.478745  [28404/60000]\n",
      "loss: 1.532627  [28804/60000]\n",
      "loss: 1.462088  [29204/60000]\n",
      "loss: 1.461204  [29604/60000]\n",
      "loss: 1.462761  [30004/60000]\n",
      "loss: 1.715251  [30404/60000]\n",
      "loss: 1.655872  [30804/60000]\n",
      "loss: 1.461486  [31204/60000]\n",
      "loss: 1.461239  [31604/60000]\n",
      "loss: 1.461590  [32004/60000]\n",
      "loss: 1.547935  [32404/60000]\n",
      "loss: 1.703683  [32804/60000]\n",
      "loss: 1.894809  [33204/60000]\n",
      "loss: 1.462873  [33604/60000]\n",
      "loss: 1.467362  [34004/60000]\n",
      "loss: 1.462817  [34404/60000]\n",
      "loss: 1.710415  [34804/60000]\n",
      "loss: 1.461937  [35204/60000]\n",
      "loss: 1.494095  [35604/60000]\n",
      "loss: 1.461185  [36004/60000]\n",
      "loss: 1.663184  [36404/60000]\n",
      "loss: 1.471281  [36804/60000]\n",
      "loss: 1.461200  [37204/60000]\n",
      "loss: 1.702381  [37604/60000]\n",
      "loss: 1.737811  [38004/60000]\n",
      "loss: 1.464700  [38404/60000]\n",
      "loss: 1.494090  [38804/60000]\n",
      "loss: 1.499673  [39204/60000]\n",
      "loss: 1.463374  [39604/60000]\n",
      "loss: 1.709429  [40004/60000]\n",
      "loss: 1.464356  [40404/60000]\n",
      "loss: 1.710850  [40804/60000]\n",
      "loss: 1.461159  [41204/60000]\n",
      "loss: 1.517474  [41604/60000]\n",
      "loss: 1.461239  [42004/60000]\n",
      "loss: 1.461161  [42404/60000]\n",
      "loss: 1.461185  [42804/60000]\n",
      "loss: 1.461369  [43204/60000]\n",
      "loss: 1.642350  [43604/60000]\n",
      "loss: 1.461218  [44004/60000]\n",
      "loss: 1.486216  [44404/60000]\n",
      "loss: 1.461214  [44804/60000]\n",
      "loss: 1.466370  [45204/60000]\n",
      "loss: 1.461880  [45604/60000]\n",
      "loss: 1.462236  [46004/60000]\n",
      "loss: 1.514540  [46404/60000]\n",
      "loss: 1.461514  [46804/60000]\n",
      "loss: 1.461169  [47204/60000]\n",
      "loss: 1.462439  [47604/60000]\n",
      "loss: 1.461222  [48004/60000]\n",
      "loss: 1.628519  [48404/60000]\n",
      "loss: 1.461583  [48804/60000]\n",
      "loss: 1.506904  [49204/60000]\n",
      "loss: 1.704245  [49604/60000]\n",
      "loss: 1.461200  [50004/60000]\n",
      "loss: 1.461236  [50404/60000]\n",
      "loss: 1.462872  [50804/60000]\n",
      "loss: 1.534101  [51204/60000]\n",
      "loss: 1.564308  [51604/60000]\n",
      "loss: 1.461193  [52004/60000]\n",
      "loss: 1.484928  [52404/60000]\n",
      "loss: 1.462886  [52804/60000]\n",
      "loss: 1.461394  [53204/60000]\n",
      "loss: 1.461536  [53604/60000]\n",
      "loss: 1.461150  [54004/60000]\n",
      "loss: 1.704822  [54404/60000]\n",
      "loss: 1.709016  [54804/60000]\n",
      "loss: 1.461644  [55204/60000]\n",
      "loss: 1.461367  [55604/60000]\n",
      "loss: 1.540901  [56004/60000]\n",
      "loss: 1.710104  [56404/60000]\n",
      "loss: 1.650730  [56804/60000]\n",
      "loss: 1.461168  [57204/60000]\n",
      "loss: 1.461881  [57604/60000]\n",
      "loss: 1.462196  [58004/60000]\n",
      "loss: 1.461223  [58404/60000]\n",
      "loss: 1.473166  [58804/60000]\n",
      "loss: 1.461172  [59204/60000]\n",
      "loss: 1.461432  [59604/60000]\n"
     ]
    }
   ],
   "source": [
    "train_loop(dataloader, tinymodel, loss_fn, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 1.524532 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(dataloader, tinymodel, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in tinymodel.parameters():\n",
    "    params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand(1, 32, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
